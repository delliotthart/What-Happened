---
title: "What Happened?" 
output: html_document     
table_of_contents: TRUE
runtime: shiny

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
suppressMessages(suppressWarnings(source('scripts/KNN.R')))

```

#K Nearest Neighbors
What is particularly interesting about the K-Nearest Neighbors Regression approach is that we can evaluate *what kinds* of similarities are most predictive of how counties vote. We have found that household income, home value, and age are among the most predictive.  

K-Nearest Neighbors also penalizes dimensionality "automatically." As we include more and more features, the distance function become less reliable for reasons both technical and substantive. Technically, distance metrics become less use in high dimensional space, as all points converge towards being [arbitrarily far](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions) from each other. 

This is not just an abstract mathematical result. As we add less informative features, the clustering algorithm will treat observations that are similar in terms of the less informative feature the same as it treats the observerations that are meaningfully similar. That is, if we add uninformative features, we end up including "bad comparisons" along with our "good comparisons." Additionally, as we increase the number of neighbors, we similarly risk including "bad comparisons," as we add more and more observations that are less similar.  



With this interactive widget, you can try to find the best attributes. Watch as the prediction  gets *worse* if you add uninformative attributes, or if you add too many neighbors! 

## K Nearest Neighbors: What Kinds of Similarities Are Most Predictive?

```{r Display_KNN_Model, echo=FALSE}
   #main_data <- get_election_data()
   # Application title
   
   # Sidebar with a slider input for number of bins 
   inputPanel(
     selectInput("features",
                     "Select Features",
                      choices = feature_names,
                     selected = 'College Degree', multiple = TRUE),
     sliderInput('K',"Number of Neighbors",
                 min = 5, max = 100, value = 20, step = 1)
      )
    

renderPlot(
    display_KNN(input$K,input$features), width = "auto", height = 450
   )

```

## What Are the Most Similar Counties?  

We can take an even closer look to see which *exact counties* are being used as the comparison group in the best performing models.  

How close can you get the predicted value to the real vote share? What features seem to work best? At what point does adding more features seem to make the model worse?

```{r one_county_take_two, echo=FALSE}
  source('scripts/KNN.R')

shinyAppDir(appDir =  "reactive_KNN_graph",
  options = list(
    width = "100%", height = 1100
  )
)
```

