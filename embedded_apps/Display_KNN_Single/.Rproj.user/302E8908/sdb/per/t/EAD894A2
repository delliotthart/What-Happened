{
    "collab_server" : "",
    "contents" : "---\ntitle: \"What Happened?\"\nresource_files:\n- reactive_KNN_graph/server.R\n- reactive_KNN_graph/ui.R\nruntime: shiny\noutput:\n  html_notebook:\n    toc: yes\n---\n\n```{r global}\nknitr::opts_chunk$set(eval = FALSE)\nlist.of.packages <- c(\"ggplot2\", \"FNN\",\"readr\",\"tidyverse\",\"HDCI\", \"shiny\", 'glmnet')\nnew.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\nif(length(new.packages)) install.packages(new.packages)\nlibrary(shiny)\n\nsuppressMessages(suppressWarnings(source('scripts/KNN.R')))\nsuppressMessages(suppressWarnings(source('scripts/lasso.R')))\nsuppressMessages(suppressWarnings(source('scripts/valueCalculator.R')))\n\n\n\n```\n\n## K Nearest Neighbors\nWhat is particularly interesting about the K-Nearest Neighbors Regression approach is that we can evaluate *what kinds* of similarities are most predictive of how counties vote. We have found that household income, home value, and age are among the most predictive.  \n\nK-Nearest Neighbors also penalizes dimensionality \"automatically.\" As we include more and more features, the distance function become less reliable for reasons both technical and substantive. Technically, distance metrics become less use in high dimensional space, as all points converge towards being [arbitrarily far](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions) from each other. \n\nThis is not just an abstract mathematical result. As we add less informative features, the clustering algorithm will treat observations that are similar in terms of the less informative feature the same as it treats the observerations that are meaningfully similar. That is, if we add uninformative features, we end up including \"bad comparisons\" along with our \"good comparisons.\" Additionally, as we increase the number of neighbors, we similarly risk including \"bad comparisons,\" as we add more and more observations that are less similar.  \n\n### K Nearest Neighbors: What Kinds of Similarities Are Most Predictive?\n\nWith this interactive widget, you can try to find the best attributes. Watch as the prediction  gets *worse* if you add uninformative attributes. You can also see that the prediction gets flatter as you increase neighbors; as we average over a larger set of counties, we regress towards the mean. When $k == N$, the prediction is simply the sample average of Y for any set of features.\n\n\n```{r Display_KNN_Model}\n   #main_data <- get_election_data()\n   # Application title\n   \n   # Sidebar with a slider input for number of bins \n   inputPanel(\n     selectInput(\"features\",\n                     \"Select Features\",\n                      choices = feature_names,\n                     selected = 'College Degree', multiple = TRUE),\n     sliderInput('K',\"Number of Neighbors\",\n                 min = 5, max = 100, value = 20, step = 1)\n      )\n    \n\nrenderPlot(\n    display_KNN(input$K,input$features), width = \"auto\", height = 450\n   )\n\n```\n\n### What Are the Most Similar Counties?  \n\nWe can take an even closer look to see which *exact counties* are being used as the comparison group in the best performing models.  \n\nHow close can you get the predicted value to the real vote share? What features seem to work best? At what point does adding more features seem to make the model worse?\n\n```{r one_county_take_two}\n  source('scripts/KNN.R')\n\nshinyAppDir(appDir =  \"reactive_KNN_graph\",\n  options = list(\n    width = \"100%\", height = 1100\n  )\n)\n```\n\n### We can also try Lasso to identify the most important features     \n\n\nIn Lasso, we can tune a parameter $\\alpha$ that allows us to penalize the absolute value of coefficients. Lasso will then \"zero out\" the least \"informative\" coefficients. This interactive allows us to see how changing the values of $\\alpha$ changes our estimated coefficents. \n\nTry to find a value of $\\alpha$ that returns a useful set of coefficients![^fn1]  At what point do you think the model starts removing \"informative\" features? \n\n[^fn1]: The values of $\\alpha$ we use actually range between 0 and .075. Since there is no innate meaning to the actual cardinal value of $\\alpha$, we choose to let users select from a range of 0 to 100, which we then appropriately rescale.\n\nNote that when we start with $\\alpha = 0$, we are running a standard OLS regression.  \n\n\n```{r lasso_plot}\n\n  sliderInput('alpha',\n              'How agressively should we penalize the magnitude of coefficients?',\n              min = 0, max = 100, step = 1, value = 0,width= '100%')\n    \n```\n\n\n```{r render_lass}\nrenderPlot(\n  lassoGivenAlpha(input$alpha*.00075)\n)\n```\n\n### Survey Selection Tool\n\nIn the real world, data isn't free. Campaigns would need to select In this model, we identify the most cost eff\n\n",
    "created" : 1557426195356.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2377127251",
    "id" : "EAD894A2",
    "lastKnownWriteTime" : 1557422893,
    "last_content_update" : 1557422893,
    "path" : "~/Dropbox (Brown)/Brown/What-Happened/shiny_app/index.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}